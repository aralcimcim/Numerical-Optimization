{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus Task\n",
    "We need ot find such a problem for which Quasi Newton method outperforms Newton method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's consider a modified version of the Himmelblau function:\n",
    "$$ f(x, y) = (3x^2 + y - 13)^2 + (x + 4y^2 - 19)^2 $$\n",
    "\n",
    "**Define the Modified Function and Its Derivatives**\n",
    "\n",
    "   - Function: $$f(x, y) = (3x^2 + y - 13)^2 + (x + 4y^2 - 19)^2 $$\n",
    "   - Gradient:\n",
    "     $$\n",
    "     \\nabla f(x, y) = \\begin{bmatrix}\n",
    "     2 \\cdot 2(3x^2 + y - 13) \\cdot 6x + 2(x + 4y^2 - 19) \\\\\n",
    "     2(3x^2 + y - 13) + 2 \\cdot 2(x + 4y^2 - 19) \\cdot 8y\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "   - Hessian:\n",
    "     $$\n",
    "     H(x, y) = \\begin{bmatrix}\n",
    "     2 \\cdot 6(3x^2 + y - 13) \\cdot 6 + 36x^2 + 2 & 6 \\\\\n",
    "     6 & 2 + 64y^2\n",
    "     \\end{bmatrix}\n",
    "     $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'Starting Point': array([1.2, 1.2]),\n  'Newton Method': {'Iterations': 999,\n   'Final Iterate': array([0.99320368, 2.16834158]),\n   'Gradient Norm': 93.00433432591915,\n   'Time': 0.2527487277984619},\n  'Quasi-Newton Method': {'Iterations': 15,\n   'Final Iterate': array([1.90900824, 2.06706264]),\n   'Gradient Norm': 8.743733958311647e-07,\n   'Time': 0.0}},\n {'Starting Point': array([1.5, 1.5]),\n  'Newton Method': {'Iterations': 999,\n   'Final Iterate': array([1.31923382, 2.23859313]),\n   'Gradient Norm': 110.9207952940795,\n   'Time': 0.2412114143371582},\n  'Quasi-Newton Method': {'Iterations': 17,\n   'Final Iterate': array([1.90900824, 2.06706264]),\n   'Gradient Norm': 5.120829881885181e-07,\n   'Time': 0.0}},\n {'Starting Point': array([-1.2,  1. ]),\n  'Newton Method': {'Iterations': 999,\n   'Final Iterate': array([-1.09767371,  2.15635183]),\n   'Gradient Norm': 113.49535466400827,\n   'Time': 0.22078776359558105},\n  'Quasi-Newton Method': {'Iterations': 16,\n   'Final Iterate': array([-1.88986142,  2.2852714 ]),\n   'Gradient Norm': 7.051928368531318e-07,\n   'Time': 0.0}},\n {'Starting Point': array([0.2, 0.8]),\n  'Newton Method': {'Iterations': 999,\n   'Final Iterate': array([0.1756172 , 2.20471207]),\n   'Gradient Norm': 21.321877702899837,\n   'Time': 0.22168183326721191},\n  'Quasi-Newton Method': {'Iterations': 16,\n   'Final Iterate': array([1.90900824, 2.06706264]),\n   'Gradient Norm': 9.045724724880107e-07,\n   'Time': 0.0}}]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define the modified Himmelblau function\n",
    "def f_modified(x):\n",
    "    return (3 * x[0]**2 + x[1] - 13)**2 + (x[0] + 4 * x[1]**2 - 19)**2\n",
    "\n",
    "# Define the gradient of the modified function\n",
    "def grad_f_modified(x):\n",
    "    df_dx = 2 * (3 * x[0]**2 + x[1] - 13) * 6 * x[0] + 2 * (x[0] + 4 * x[1]**2 - 19)\n",
    "    df_dy = 2 * (3 * x[0]**2 + x[1] - 13) + 2 * (x[0] + 4 * x[1]**2 - 19) * 8 * x[1]\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Define the Hessian of the modified function\n",
    "def hessian_f_modified(x):\n",
    "    d2f_dx2 = 2 * 6 * (3 * x[0]**2 + x[1] - 13) * 6 + 36 * x[0]**2 + 2\n",
    "    d2f_dy2 = 2 + 64 * x[1]**2\n",
    "    d2f_dxdy = 6\n",
    "    return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n",
    "\n",
    "# Backtracking line search\n",
    "def backtracking_line_search(f, x, grad, p, alpha=0.3, beta=0.5):\n",
    "    t = 1.0\n",
    "    while f(x + t * p) > f(x) + alpha * t * np.dot(grad, p):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "# Newton method\n",
    "def newton_method(f, grad, hessian, x0, max_iter=1000, tol=1e-6):\n",
    "    x = x0.copy()\n",
    "    for i in range(max_iter):\n",
    "        grad_x = grad(x)\n",
    "        if np.linalg.norm(grad_x) < tol:\n",
    "            break\n",
    "        hess_x = hessian(x)\n",
    "        try:\n",
    "            p = np.linalg.solve(hess_x, -grad_x)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break  # In case Hessian is singular, break\n",
    "        t = backtracking_line_search(f, x, grad_x, p)\n",
    "        x += t * p\n",
    "    return x, np.linalg.norm(grad_x), i\n",
    "\n",
    "# BFGS update\n",
    "def bfgs_update(H, s, y):\n",
    "    ys = np.dot(y, s)\n",
    "    if ys < 1e-10:  # Prevent division by zero or very small values\n",
    "        return H\n",
    "    rho = 1.0 / ys\n",
    "    I = np.eye(len(H))\n",
    "    V = I - rho * np.outer(s, y)\n",
    "    H = V.T @ H @ V + rho * np.outer(s, s)\n",
    "    return H\n",
    "\n",
    "# Quasi-Newton method\n",
    "def quasi_newton_method(f, grad, x0, max_iter=1000, tol=1e-6):\n",
    "    x = x0.copy()\n",
    "    n = len(x)\n",
    "    H = np.eye(n)\n",
    "    for i in range(max_iter):\n",
    "        grad_x = grad(x)\n",
    "        if np.linalg.norm(grad_x) < tol:\n",
    "            break\n",
    "        p = -np.dot(H, grad_x)\n",
    "        t = backtracking_line_search(f, x, grad_x, p)\n",
    "        s = t * p\n",
    "        x_next = x + s\n",
    "        y = grad(x_next) - grad_x\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            H = bfgs_update(H, s, y)\n",
    "        x = x_next\n",
    "    return x, np.linalg.norm(grad(x)), i\n",
    "\n",
    "# Initial points for testing\n",
    "x0_list_modified = [(1.2, 1.2), (1.5, 1.5), (-1.2, 1.0), (0.2, 0.8)]\n",
    "\n",
    "results_modified = []\n",
    "\n",
    "for i, x0 in enumerate(x0_list_modified):\n",
    "    x0 = np.array(x0)\n",
    "\n",
    "    start_time = time.time()\n",
    "    x_newton, grad_norm_newton, num_iter_newton = newton_method(f_modified, grad_f_modified, hessian_f_modified, x0)\n",
    "    time_newton = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    x_qn, grad_norm_qn, num_iter_qn = quasi_newton_method(f_modified, grad_f_modified, x0)\n",
    "    time_qn = time.time() - start_time\n",
    "\n",
    "    results_modified.append({\n",
    "        \"Starting Point\": x0,\n",
    "        \"Newton Method\": {\"Iterations\": num_iter_newton, \"Final Iterate\": x_newton, \"Gradient Norm\": grad_norm_newton, \"Time\": time_newton},\n",
    "        \"Quasi-Newton Method\": {\"Iterations\": num_iter_qn, \"Final Iterate\": x_qn, \"Gradient Norm\": grad_norm_qn, \"Time\": time_qn}\n",
    "    })\n",
    "\n",
    "results_modified"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-17T22:40:31.884171Z",
     "end_time": "2024-05-17T22:40:32.958487Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "1. **Starting Point: (1.2, 1.2)**\n",
    "   - **Newton Method**\n",
    "     - Iterations: 999\n",
    "     - Final Iterate: [0.99320368, 2.16834158]\n",
    "     - Gradient Norm: 93.0043\n",
    "     - Time: 0.459 seconds\n",
    "   - **Quasi-Newton Method**\n",
    "     - Iterations: 15\n",
    "     - Final Iterate: [1.90900824, 2.06706264]\n",
    "     - Gradient Norm: 8.74e-07\n",
    "     - Time: 0.0009 seconds\n",
    "\n",
    "2. **Starting Point: (1.5, 1.5)**\n",
    "   - **Newton Method**\n",
    "     - Iterations: 999\n",
    "     - Final Iterate: [1.31923382, 2.23859313]\n",
    "     - Gradient Norm: 110.921\n",
    "     - Time: 0.494 seconds\n",
    "   - **Quasi-Newton Method**\n",
    "     - Iterations: 17\n",
    "     - Final Iterate: [1.90900824, 2.06706264]\n",
    "     - Gradient Norm: 5.12e-07\n",
    "     - Time: 0.0012 seconds\n",
    "\n",
    "3. **Starting Point: (-1.2, 1.0)**\n",
    "   - **Newton Method**\n",
    "     - Iterations: 999\n",
    "     - Final Iterate: [-1.09767371, 2.15635183]\n",
    "     - Gradient Norm: 113.495\n",
    "     - Time: 0.527 seconds\n",
    "   - **Quasi-Newton Method**\n",
    "     - Iterations: 16\n",
    "     - Final Iterate: [-1.88986142, 2.2852714]\n",
    "     - Gradient Norm: 7.05e-07\n",
    "     - Time: 0.0011 seconds\n",
    "\n",
    "4. **Starting Point: (0.2, 0.8)**\n",
    "   - **Newton Method**\n",
    "     - Iterations: 999\n",
    "     - Final Iterate: [0.1756172, 2.20471207]\n",
    "     - Gradient Norm: 21.322\n",
    "     - Time: 0.463 seconds\n",
    "   - **Quasi-Newton Method**\n",
    "     - Iterations: 16\n",
    "     - Final Iterate: [1.90900824, 2.06706264]\n",
    "     - Gradient Norm: 9.05e-07\n",
    "     - Time: 0.0013 seconds\n",
    "\n",
    "### Summary\n",
    "For all the starting points tested, the Quasi-Newton method (BFGS) consistently converged faster (in fewer iterations and significantly less time) compared to the Newton method. The Newton method struggled to converge within 1000 iterations and did not significantly reduce the gradient norm, whereas the Quasi-Newton method achieved convergence with a very small gradient norm efficiently.\n",
    "\n",
    "This demonstrates a scenario where the QN  outperforms the NM."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
