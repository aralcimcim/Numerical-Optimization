{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 19, 37, 68] [5, 8, 12, 20]\n"
     ]
    }
   ],
   "source": [
    "def cgsolve(A, b, x0, maxiters, tol):\n",
    "    x = x0.copy()\n",
    "    r = A @ x - b\n",
    "    p = -r\n",
    "    Ap = A @ p\n",
    "    init_res_norm = r.T @ r\n",
    "    alpha = init_res_norm / (p.T @ Ap)\n",
    "    niters = 0\n",
    "    while (np.sqrt(init_res_norm) / np.linalg.norm(b) > tol) and (niters < maxiters):\n",
    "        niters += 1\n",
    "        x += alpha * p\n",
    "        r += alpha * Ap\n",
    "        norm_old = init_res_norm\n",
    "        init_res_norm = r.T @ r\n",
    "        beta = init_res_norm / norm_old\n",
    "        p = -r + beta * p\n",
    "        Ap = A @ p\n",
    "        alpha = init_res_norm / (p.T @ Ap)\n",
    "\n",
    "    return x, niters, init_res_norm\n",
    "\n",
    "allNs = [5, 8, 12, 20]\n",
    "allNIters = []\n",
    "for n in allNs:\n",
    "    A = hilbert(n)\n",
    "    b = np.ones((n, 1))\n",
    "    x0 = np.zeros((n, 1))\n",
    "    x, niter, init_res_norm = cgsolve(A, b, x0, 100, 1e-6)\n",
    "    allNIters.append(niter)\n",
    "\n",
    "print(allNIters, allNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of iters: [6, 19, 38, 67] | Corresponding dims: [5, 8, 12, 20]\n"
     ]
    }
   ],
   "source": [
    "def conjugate_gradient(A, b, x_0, n, tol=1e-6, max_iters=100**6):\n",
    "    r = A @ x_0 - b\n",
    "    p = -r\n",
    "    x_k = 0\n",
    "    r_k = 0\n",
    "    k = 0\n",
    "\n",
    "    while k < max_iters and np.linalg.norm(r) > tol:\n",
    "        alpha = (r.T @ r) / (p.T @ A @ p)\n",
    "        x_k = x_0 + alpha * p\n",
    "        r_k = r + alpha * A @ p\n",
    "        beta = (r_k.T @ r_k) / (r.T @ r)\n",
    "        p_k = -r_k + beta * p\n",
    "\n",
    "        p = p_k\n",
    "        r = r_k\n",
    "        x_0 = x_k\n",
    "        k += 1\n",
    "\n",
    "    return x_k, r_k, k\n",
    "\n",
    "dimensions = [5, 8, 12, 20]\n",
    "iterations = []\n",
    "\n",
    "for n in dimensions:\n",
    "    A = hilbert(n)\n",
    "    b = np.ones(n)\n",
    "    x_0 = np.zeros(n)\n",
    "    x, r, num_iter = conjugate_gradient(A, b, x_0, n)\n",
    "    iterations.append(num_iter)\n",
    "\n",
    "print(f'Num of iters: {iterations} | Corresponding dims: {dimensions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[[162 282 330]\n",
      " [150 292 266]\n",
      " [114 182 370]]\n"
     ]
    }
   ],
   "source": [
    "p_i = np.array([[1, 4, 7],\n",
    "                [2, 1, 8],\n",
    "                [3, 6, 1]])\n",
    "\n",
    "p_j = np.array([[1, 2, 3],\n",
    "                [4, 1, 6],\n",
    "                [7, 8, 1]])\n",
    "\n",
    "A = np.array([[4, 1, 1], \n",
    "              [1, 4, 1], \n",
    "              [1, 1, 4]])\n",
    "\n",
    "print(np.all(np.linalg.eigvals(A) > 0))\n",
    "\n",
    "#p_j^T * A * p_i\n",
    "result = np.dot(p_j.T, np.dot(A, p_i))\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jku_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
